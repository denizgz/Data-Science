{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project, Data Science 2020\n",
    "\n",
    "## Overview\n",
    "\n",
    "The goal of this project is to take your first dive into **interpretable ML**.\n",
    "As would be the case for most industrial projects with companies, your expertise at this point is not quite sufficient and part of your job is to get up to speed with new concepts and code.\n",
    "So, as a first step I recommend the following \n",
    "\n",
    "* Understand the basic idea of random forests: read chapters 8.2.1 and 8.2.2 in our ISLR text book.\n",
    "* Imbibe the basic tools of model interpretation such as partial dependence plots, permutation importance and SHAP values: read 5.1, 5.5, 5.9-5.10 in the [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/).\n",
    "\n",
    "The data set I want you to tackle comes from this [kaggle competition](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries), so please join the competition and explore some of the notebooks. Keep in mind though that I allow only the following models \n",
    "\n",
    "* regularized linear models (lasso or ridge or enet)\n",
    "* trees\n",
    "* random forests\n",
    "\n",
    "Note that the focus is not just prediction! (It would indeed be too easy to just copy an existing notebook with good performance)\n",
    "You will be graded on 4 aspects of the \"modeling journey\", all of which are typically equally valuable when you deliver a data science task to a company:\n",
    "\n",
    "1. **Modeling Diligence**: this includes data preprocessing, dealing with mising values and categorial variables, feature engineering, choosing appropriate tools (e.g. cross validation), general creativity, ...\n",
    "2. **Performance**: using an appropriate loss function on a test set defined below. Note that -given similar performance- \"less complex\" models (either in number of features or tree depth or ...) are judged to be more interpretable.\n",
    "3. **Explaining predictions**, both globally (e.g. permutation importance) and case-by-case via SHAP values.\n",
    "4. **Story Telling** via compelling graphs (e.g. PDPs or interaction SHAP plots) and \"cherry picked\" examples of e.g. local explanations which either make perfect sense or might violate intuition.\n",
    "\n",
    "You can choose from one of two  prediction tasks (or both):\n",
    "\n",
    "* predict number of inquiries a new listing receives\n",
    "* predict apartment rent\n",
    "\n",
    "While it is OK to somewhat ignore the (large) image data provided but you might want to think about simple features derived that do not need any actual image processing.\n",
    "\n",
    "As I do not want to stifle your creativity and leave you lots of room for your own ideas, I am not goinng to specify the deliverables much more than the following:\n",
    "\n",
    "- You will have to present your findings as a group sometime early February.\n",
    "- In addition you need to prepare a report which could be just a Jupyter notebook with a sufficient amount of text.\n",
    "\n",
    "\n",
    "For inspiration, clever tricks and data wrangling and to get you started on this data set in general, I recommend [The Mechanics of Machine Learning](https://mlbook.explained.ai/) by Terence Parr and Jeremy Howard. Keep in mind that they are working on a reduced number of features.\n",
    "\n",
    "Also: if you have *computational resources issues*, i.e. if kaggle notebooks or google colab or MS azure turn out to be too slow or too buggy, we will attempt to give you access to our university server zeno.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validate, Test \n",
    "\n",
    "For the first time we will split our data into **three parts** instead of just two: The **train** set is for training the model, the **validation** set is to evaluate model performance out-of-training during hyperparameter tuning, and the **test** set is used only once for the final performance estimate of the model on unseen data.\n",
    "\n",
    "You should read [9 Train, Validate, Test](https://mlbook.explained.ai/bulldozer-testing.html) to fully appreciate the subtle issues involved int this \"trilogy\". \n",
    "\n",
    "Please set your random seed to 123 and divide your data into (roughly) 70% training and 15% into both validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(123)\n",
    "df = df.sample(frac=1) # shuffle data\n",
    "df_dev, df_test = train_test_split(df, test_size=0.15)\n",
    "df_train, df_valid = train_test_split(df_dev, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Work guidelines\n",
    "\n",
    "* Make sure each team member is contributing, both in terms of quality and quantity of contribution\n",
    "* You are to complete the assignment as a team. All team members are expected to contribute equally to the completion of this assignment and team evaluations will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n",
    "\n",
    "* Team peer evaluation: You will be asked to fill out a survey where you rate the contribution and teamwork of each team member out of 10 points. You will additionally report a contribution percentage for each team member. Filling out the survey is a prerequisite for getting credit on the team member evaluation.If you are suggesting that an individual did less than $1/K$ (K= group size) of the work, please provide some explanation. If any individual gets an average peer score indicating that they did less than 10% of the work, this person will receive half the grade of the rest of the group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "There is a document in the cloud folder (Assessment-Scheme-for-Group-Presentations_Projects.pdf) which explains the evaluation metric in detail. (Note that some of the finer presentation points might not be applicable to online presentations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
